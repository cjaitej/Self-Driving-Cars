{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2040769-0bb7-4ed1-b41f-85ce1ff64cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T05:40:01.598062Z",
     "iopub.status.busy": "2024-01-27T05:40:01.597391Z",
     "iopub.status.idle": "2024-01-27T05:40:01.606420Z",
     "shell.execute_reply": "2024-01-27T05:40:01.605439Z",
     "shell.execute_reply.started": "2024-01-27T05:40:01.598034Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e578b99-2e2e-44e3-980f-b94f73fde939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T05:40:02.548410Z",
     "iopub.status.busy": "2024-01-27T05:40:02.548034Z",
     "iopub.status.idle": "2024-01-27T05:40:38.298231Z",
     "shell.execute_reply": "2024-01-27T05:40:38.297295Z",
     "shell.execute_reply.started": "2024-01-27T05:40:02.548385Z"
    }
   },
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile('images.zip')\n",
    "zip_ref.extractall(\"/images\")\n",
    "zip_ref.close()\n",
    "\n",
    "zip_ref = zipfile.ZipFile('depth.zip')\n",
    "zip_ref.extractall(\"/depth\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d0a68c-252e-445e-8b9c-b75466f31430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T05:40:38.300499Z",
     "iopub.status.busy": "2024-01-27T05:40:38.299763Z",
     "iopub.status.idle": "2024-01-27T05:40:42.023256Z",
     "shell.execute_reply": "2024-01-27T05:40:42.022204Z",
     "shell.execute_reply.started": "2024-01-27T05:40:38.300475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting piqa\n",
      "  Downloading piqa-1.3.2-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from piqa) (1.12.1+cu116)\n",
      "Requirement already satisfied: torchvision>=0.13.0 in /usr/local/lib/python3.9/dist-packages (from piqa) (0.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.12.0->piqa) (4.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.13.0->piqa) (1.23.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.13.0->piqa) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.13.0->piqa) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.13.0->piqa) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision>=0.13.0->piqa) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision>=0.13.0->piqa) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.13.0->piqa) (1.26.14)\n",
      "Installing collected packages: piqa\n",
      "Successfully installed piqa-1.3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Depth Code:\n",
    "!pip install piqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac33009e-b923-4bf5-8aea-dd9563c5d9e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T05:40:42.025923Z",
     "iopub.status.busy": "2024-01-27T05:40:42.025625Z",
     "iopub.status.idle": "2024-01-27T05:40:43.357903Z",
     "shell.execute_reply": "2024-01-27T05:40:43.356899Z",
     "shell.execute_reply.started": "2024-01-27T05:40:42.025879Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def create_json_dataset(image_dir, mask_dir):\n",
    "    for folder_name in os.listdir(image_dir):\n",
    "        image_path = image_dir + \"/\" + folder_name\n",
    "        mask_path = mask_dir + \"/\" + folder_name\n",
    "        data = []\n",
    "        for sub_folder in os.listdir(image_path):\n",
    "            for image in os.listdir(image_path + \"/\" + sub_folder):\n",
    "                image_name = image_path + \"/\" + sub_folder + \"/\" + image\n",
    "                mask_name = mask_path + \"/\" + sub_folder + \"/\" + image[:-15] + \"depth.png\"\n",
    "                data.append([image_name, mask_name])\n",
    "\n",
    "        with open(f'{folder_name}.json', \"w\", encoding='utf-8') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "def transform(image, mask):\n",
    "    hflip = transforms.RandomHorizontalFlip(p=1)\n",
    "    vflip = transforms.RandomVerticalFlip(p=1)\n",
    "    totensor = transforms.PILToTensor()\n",
    "\n",
    "    if random.random() > 0.5:\n",
    "        image = hflip(image)\n",
    "        mask = hflip(mask)\n",
    "\n",
    "    # #Vertical Flipping\n",
    "    if random.random() > 0.5:\n",
    "        image = vflip(image)\n",
    "        mask = vflip(mask)\n",
    "\n",
    "    image = totensor(image)\n",
    "    mask = totensor(mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def add_result(result, version):\n",
    "    with open(f'results_v{version}.txt', 'a') as f:\n",
    "        f.write(result + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "def save_checkpoint(epoch, model, version):\n",
    "    state = {'epoch': epoch,\n",
    "             'model': model}\n",
    "    filename = f'Depth_v{version}.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "def draw_loss_graph(file_name, save_name, from_epoch = 0):\n",
    "    if file_name.startswith(\"http\"):\n",
    "        f = urlopen(file_name).read().decode('utf-8').split(\"\\n\")[:-1]\n",
    "    else:\n",
    "        f = open(file_name, 'r')\n",
    "        f = f.readlines()\n",
    "    x = []\n",
    "    y = []\n",
    "    y_val = []\n",
    "    for i in f[from_epoch:]:\n",
    "        temp = i.split(\" \")\n",
    "        x.append(int(temp[1]))\n",
    "        y.append(float(temp[5]))\n",
    "        y_val.append(float(temp[-1].replace(\"\\n\", \"\")))\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(x, y, color='c', label='Train Loss')\n",
    "    plt.plot(x, y_val, color='orange',label='Val Loss')\n",
    "    plt.ticklabel_format(useOffset=False, style='plain')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_name)\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb63df9e-adcf-4778-9674-2d785b51a825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T05:40:43.360300Z",
     "iopub.status.busy": "2024-01-27T05:40:43.359928Z",
     "iopub.status.idle": "2024-01-27T05:40:43.371332Z",
     "shell.execute_reply": "2024-01-27T05:40:43.370529Z",
     "shell.execute_reply.started": "2024-01-27T05:40:43.360274Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class CityScapeDepth(Dataset):\n",
    "    def __init__(self, filename, size = (512, 256),transform=None):\n",
    "        super(CityScapeDepth, self).__init__()\n",
    "        f = open(filename)\n",
    "        self.data = json.load(f)\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, mask_path = self.data[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\").resize(self.size)\n",
    "        mask = Image.open(mask_path).convert('L').resize(self.size, Image.Resampling.NEAREST)\n",
    "\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        image = image / 255.\n",
    "        mask = mask / 255.\n",
    "\n",
    "        image_stack = torch.zeros(2, 3, self.size[1], self.size[0]//2)\n",
    "        mask_stack = torch.zeros(2, 1, self.size[1], self.size[0]//2)\n",
    "\n",
    "        image_stack[0] = image[:, :, :self.size[0]//2]\n",
    "        image_stack[1] = image[:, :, self.size[0]//2:]\n",
    "\n",
    "        mask_stack[0] = mask[:, :, :self.size[0]//2]\n",
    "        mask_stack[1] = mask[:, :, self.size[0]//2:]\n",
    "        return image_stack.to(torch.float32), mask_stack.to(torch.float32)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images = []\n",
    "        masks = []\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            masks.append(b[1])\n",
    "\n",
    "        images = torch.cat(images, dim=0)\n",
    "        masks = torch.cat(masks, dim=0)\n",
    "\n",
    "        return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d193ab-fb1f-468d-84da-120d4417bb3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T05:40:43.372842Z",
     "iopub.status.busy": "2024-01-27T05:40:43.372547Z",
     "iopub.status.idle": "2024-01-27T05:40:43.403186Z",
     "shell.execute_reply": "2024-01-27T05:40:43.402217Z",
     "shell.execute_reply.started": "2024-01-27T05:40:43.372818Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from piqa import SSIM\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, g_in_c, x_in_c):\n",
    "        super(AttentionGate, self).__init__()\n",
    "\n",
    "        self.g_conv_layer = nn.Conv2d(g_in_c, x_in_c, 1, 1)\n",
    "        self.x_conv_layer = nn.Conv2d(x_in_c, x_in_c, 1, 2)\n",
    "        self.si_conv_layer = nn.Conv2d(x_in_c*2, 1, 1, 1)\n",
    "        self.resampling = nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g = self.g_conv_layer(g)\n",
    "        g = torch.cat([g, self.x_conv_layer(x)], dim=1)\n",
    "        g = nn.ReLU()(g)\n",
    "        g = self.si_conv_layer(g)\n",
    "        g = nn.Sigmoid()(g)\n",
    "        g = self.resampling(g)\n",
    "        x = x*g\n",
    "        return x\n",
    "\n",
    "class ConvLayers(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(ConvLayers, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_c + in_c, out_c, 3, padding=1)\n",
    "        self.batchNorm = nn.BatchNorm2d(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = torch.cat([y, x], dim=1)\n",
    "        y = self.conv2(y)\n",
    "        y = self.batchNorm(y)\n",
    "        return nn.ReLU()(y)\n",
    "\n",
    "class DownSampling(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(DownSampling, self).__init__()\n",
    "        self.conv1 = ConvLayers(in_c=in_c, out_c=out_c)\n",
    "        self.conv2 = ConvLayers(in_c=out_c, out_c=out_c)\n",
    "        self.dropout = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x, self.dropout(nn.MaxPool2d(2)(x))\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(UpSampling, self).__init__()\n",
    "        self.attention_layer = AttentionGate(in_c, out_c)\n",
    "        self.upsampling_layer = nn.Upsample(scale_factor=2)\n",
    "        self.conv_layer = ConvLayers(in_c + out_c, out_c)\n",
    "        self.dropout = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, x, intermediate_value):\n",
    "        intermediate_value = self.attention_layer(x, intermediate_value)\n",
    "        x = self.upsampling_layer(x)\n",
    "        x = torch.cat([x, intermediate_value], dim=1)\n",
    "        return self.dropout(self.conv_layer(x))\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(UNET, self).__init__()\n",
    "        self.layer1 = DownSampling(in_c, 32)\n",
    "        self.downLayers = nn.ModuleList([DownSampling(2**i, 2**(i + 1)) for i in range(5, 8)])\n",
    "        self.intermediate_layer = ConvLayers(2**(8), 2**(9))\n",
    "        self.upLayers = nn.ModuleList([UpSampling(2**i, 2**(i -1)) for i in range(9, 5, -1)])\n",
    "        self.final_layer = nn.Conv2d(32, out_channels=out_c, kernel_size=1)\n",
    "        self.activation_layer = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_values = []\n",
    "        i, x = self.layer1(x)\n",
    "        intermediate_values.append(i)\n",
    "        for layer in self.downLayers:\n",
    "            i, x = layer(x)\n",
    "            intermediate_values.append(i)\n",
    "        x = self.intermediate_layer(x)\n",
    "\n",
    "        for layer, i in zip(self.upLayers, intermediate_values[::-1]):\n",
    "            x = layer(x, i)\n",
    "\n",
    "        x = self.final_layer(x)\n",
    "        return self.activation_layer(x)\n",
    "\n",
    "\n",
    "class DepthEstimationLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepthEstimationLoss, self).__init__()\n",
    "        self.mse_loss_layer = torch.nn.MSELoss()\n",
    "        self.smooth_l1_loss_layer = torch.nn.SmoothL1Loss()\n",
    "        self.ssim = SSIM(n_channels=1)\n",
    "\n",
    "    def forward(self, predicted_depth, ground_truth_depth):\n",
    "        MSE_loss = self.mse_loss_layer(predicted_depth, ground_truth_depth)\n",
    "        smooth_l1_loss = self.smooth_l1_loss_layer(predicted_depth, ground_truth_depth)\n",
    "        ssim_loss = (1. - self.ssim(predicted_depth, ground_truth_depth))/2\n",
    "        return MSE_loss + smooth_l1_loss + ssim_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40478420-2a3c-4be8-9acd-9d4063b782aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T17:39:21.707763Z",
     "iopub.status.busy": "2024-01-23T17:39:21.707557Z",
     "iopub.status.idle": "2024-01-23T17:39:21.724472Z",
     "shell.execute_reply": "2024-01-23T17:39:21.723894Z",
     "shell.execute_reply.started": "2024-01-23T17:39:21.707744Z"
    }
   },
   "outputs": [],
   "source": [
    "# from utils import create_json_dataset\n",
    "\n",
    "image_folder_path = \"/images\"\n",
    "depth_folder_path = \"/depth\"\n",
    "\n",
    "create_json_dataset(image_folder_path, depth_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d51e3-20f3-4ddc-852b-bf09aa578ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-27T05:40:43.405213Z",
     "iopub.status.busy": "2024-01-27T05:40:43.404962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Initiating the Training Process -- Version: 9\n",
      "Epoch: 1849: \n",
      "=======   Epoch: 1849 | Average Loss: 0.012890719161495522 | Val Loss: 0.020943893186215843 Sat Jan 27 05:44:34 2024\n",
      "=======   Epoch: 1850 | Average Loss: 0.012920495347764197 | Val Loss: 0.020802807062864304 Sat Jan 27 05:48:06 2024\n",
      "=======   Epoch: 1851 | Average Loss: 0.012915906141824413 | Val Loss: 0.020773999941801385 Sat Jan 27 05:51:37 2024\n",
      "=======   Epoch: 1852 | Average Loss: 0.01292766311017408 | Val Loss: 0.020853106481289223 Sat Jan 27 05:55:04 2024\n",
      "=======   Epoch: 1853 | Average Loss: 0.012935121096332988 | Val Loss: 0.020896491245366633 Sat Jan 27 05:58:36 2024\n",
      "=======   Epoch: 1854 | Average Loss: 0.012920632693927814 | Val Loss: 0.021069755162378506 Sat Jan 27 06:02:08 2024\n",
      "=======   Epoch: 1855 | Average Loss: 0.012959989890726312 | Val Loss: 0.02088683855254203 Sat Jan 27 06:05:35 2024\n",
      "=======   Epoch: 1856 | Average Loss: 0.012896178408421778 | Val Loss: 0.020828046087574745 Sat Jan 27 06:09:06 2024\n",
      "=======   Epoch: 1857 | Average Loss: 0.012903805287537979 | Val Loss: 0.020915058092214167 Sat Jan 27 06:12:38 2024\n",
      "=======   Epoch: 1858 | Average Loss: 0.012919282338095216 | Val Loss: 0.020651810785888562 Sat Jan 27 06:16:06 2024\n",
      "=======   Epoch: 1859 | Average Loss: 0.01290143025740756 | Val Loss: 0.020825014549440572 Sat Jan 27 06:19:37 2024\n",
      "=======   Epoch: 1860 | Average Loss: 0.012935860217562975 | Val Loss: 0.02083543162526829 Sat Jan 27 06:23:08 2024\n",
      "=======   Epoch: 1861 | Average Loss: 0.012930045779400542 | Val Loss: 0.020758303148405894 Sat Jan 27 06:26:36 2024\n",
      "=======   Epoch: 1862 | Average Loss: 0.012892907316546066 | Val Loss: 0.02072892672315772 Sat Jan 27 06:30:08 2024\n",
      "=======   Epoch: 1863 | Average Loss: 0.012907720483231761 | Val Loss: 0.020807422564498017 Sat Jan 27 06:33:41 2024\n",
      "=======   Epoch: 1864 | Average Loss: 0.012913601385750258 | Val Loss: 0.020791416904622956 Sat Jan 27 06:37:08 2024\n",
      "=======   Epoch: 1865 | Average Loss: 0.012935094909568028 | Val Loss: 0.020862278728080646 Sat Jan 27 06:40:40 2024\n",
      "=======   Epoch: 1866 | Average Loss: 0.01290557244680034 | Val Loss: 0.02092752421075212 Sat Jan 27 06:44:11 2024\n",
      "=======   Epoch: 1867 | Average Loss: 0.012997273141884012 | Val Loss: 0.0207908335807068 Sat Jan 27 06:47:39 2024\n",
      "=======   Epoch: 1868 | Average Loss: 0.012851444075835076 | Val Loss: 0.020736574095540812 Sat Jan 27 06:51:11 2024\n",
      "=======   Epoch: 1869 | Average Loss: 0.01289347098616945 | Val Loss: 0.02076944725454918 Sat Jan 27 06:54:42 2024\n",
      "=======   Epoch: 1870 | Average Loss: 0.012894296070442639 | Val Loss: 0.02097003228430237 Sat Jan 27 06:58:09 2024\n",
      "=======   Epoch: 1871 | Average Loss: 0.012886512565941428 | Val Loss: 0.020811589085496962 Sat Jan 27 07:01:40 2024\n",
      "=======   Epoch: 1872 | Average Loss: 0.012857099273312397 | Val Loss: 0.020985049661248922 Sat Jan 27 07:05:12 2024\n",
      "=======   Epoch: 1873 | Average Loss: 0.012916052165325854 | Val Loss: 0.02087633779072868 Sat Jan 27 07:08:40 2024\n",
      "=======   Epoch: 1874 | Average Loss: 0.01288692164727208 | Val Loss: 0.020861346474183456 Sat Jan 27 07:12:12 2024\n",
      "=======   Epoch: 1875 | Average Loss: 0.012857756993854334 | Val Loss: 0.020902036838898703 Sat Jan 27 07:15:43 2024\n",
      "=======   Epoch: 1876 | Average Loss: 0.01282058982319252 | Val Loss: 0.020795576224502708 Sat Jan 27 07:19:10 2024\n",
      "=======   Epoch: 1877 | Average Loss: 0.012838109193814305 | Val Loss: 0.0208126862500129 Sat Jan 27 07:22:41 2024\n",
      "=======   Epoch: 1878 | Average Loss: 0.012862218942020505 | Val Loss: 0.020977417971672758 Sat Jan 27 07:26:13 2024\n",
      "=======   Epoch: 1879 | Average Loss: 0.01288225995855029 | Val Loss: 0.020915813078837737 Sat Jan 27 07:29:40 2024\n",
      "=======   Epoch: 1880 | Average Loss: 0.012815875421894371 | Val Loss: 0.020824566666435982 Sat Jan 27 07:33:12 2024\n",
      "=======   Epoch: 1881 | Average Loss: 0.012834696229997448 | Val Loss: 0.02083933656103909 Sat Jan 27 07:36:43 2024\n",
      "=======   Epoch: 1882 | Average Loss: 0.012852858633838391 | Val Loss: 0.020842929736578038 Sat Jan 27 07:40:10 2024\n",
      "=======   Epoch: 1883 | Average Loss: 0.012840602304955264 | Val Loss: 0.020884506703753556 Sat Jan 27 07:43:42 2024\n",
      "=======   Epoch: 1884 | Average Loss: 0.012827076238729172 | Val Loss: 0.02095271651965699 Sat Jan 27 07:47:13 2024\n",
      "=======   Epoch: 1885 | Average Loss: 0.01285699540692919 | Val Loss: 0.02082251910386341 Sat Jan 27 07:50:40 2024\n",
      "=======   Epoch: 1886 | Average Loss: 0.01284126866485057 | Val Loss: 0.020906960225797126 Sat Jan 27 07:54:12 2024\n",
      "=======   Epoch: 1887 | Average Loss: 0.012828782316173492 | Val Loss: 0.02081572884760265 Sat Jan 27 07:57:43 2024\n",
      "=======   Epoch: 1888 | Average Loss: 0.01283553821172599 | Val Loss: 0.020685666540105428 Sat Jan 27 08:01:10 2024\n",
      "=======   Epoch: 1889 | Average Loss: 0.012848209474359574 | Val Loss: 0.020813769361536418 Sat Jan 27 08:04:42 2024\n",
      "=======   Epoch: 1890 | Average Loss: 0.012855397478263723 | Val Loss: 0.020914741874938563 Sat Jan 27 08:08:14 2024\n",
      "=======   Epoch: 1891 | Average Loss: 0.012816485023165397 | Val Loss: 0.0209514694288373 Sat Jan 27 08:11:42 2024\n",
      "=======   Epoch: 1892 | Average Loss: 0.012797520819522464 | Val Loss: 0.020754059560463896 Sat Jan 27 08:15:13 2024\n",
      "=======   Epoch: 1893 | Average Loss: 0.012815126744056757 | Val Loss: 0.02081517206637987 Sat Jan 27 08:18:44 2024\n",
      "=======   Epoch: 1894 | Average Loss: 0.012824682836783797 | Val Loss: 0.020775654802231917 Sat Jan 27 08:22:11 2024\n",
      "=======   Epoch: 1895 | Average Loss: 0.012814496541142554 | Val Loss: 0.020824273233301938 Sat Jan 27 08:25:43 2024\n",
      "=======   Epoch: 1896 | Average Loss: 0.012840506253522059 | Val Loss: 0.020931220962665975 Sat Jan 27 08:29:14 2024\n",
      "=======   Epoch: 1897 | Average Loss: 0.012832635690055764 | Val Loss: 0.020890104700811207 Sat Jan 27 08:32:42 2024\n",
      "=======   Epoch: 1898 | Average Loss: 0.012768483333841101 | Val Loss: 0.02078104283594127 Sat Jan 27 08:36:15 2024\n",
      "=======   Epoch: 1899 | Average Loss: 0.012821632789115892 | Val Loss: 0.0208120567424755 Sat Jan 27 08:39:46 2024\n",
      "=======   Epoch: 1900 | Average Loss: 0.012819386401081464 | Val Loss: 0.020830467077238218 Sat Jan 27 08:43:13 2024\n",
      "=======   Epoch: 1901 | Average Loss: 0.012796573149563683 | Val Loss: 0.02077382364742724 Sat Jan 27 08:46:45 2024\n",
      "=======   Epoch: 1902 | Average Loss: 0.012807107803459615 | Val Loss: 0.020908734844332293 Sat Jan 27 08:50:17 2024\n",
      "=======   Epoch: 1903 | Average Loss: 0.012796706967393438 | Val Loss: 0.020743636015270437 Sat Jan 27 08:53:44 2024\n",
      "=======   Epoch: 1904 | Average Loss: 0.012816411949514082 | Val Loss: 0.020703112806326578 Sat Jan 27 08:57:15 2024\n",
      "=======   Epoch: 1905 | Average Loss: 0.012778803835597643 | Val Loss: 0.020790782890149524 Sat Jan 27 09:00:46 2024\n",
      "=======   Epoch: 1906 | Average Loss: 0.012816437247161418 | Val Loss: 0.02068501998603876 Sat Jan 27 09:04:14 2024\n",
      "=======   Epoch: 1907 | Average Loss: 0.01280831817672602 | Val Loss: 0.020761013363621066 Sat Jan 27 09:07:46 2024\n",
      "=======   Epoch: 1908 | Average Loss: 0.01284047176222967 | Val Loss: 0.02077132209004568 Sat Jan 27 09:11:17 2024\n",
      "=======   Epoch: 1909 | Average Loss: 0.012836925187387315 | Val Loss: 0.02097178524958768 Sat Jan 27 09:14:45 2024\n",
      "=======   Epoch: 1910 | Average Loss: 0.01278235615312693 | Val Loss: 0.020823067985475063 Sat Jan 27 09:18:16 2024\n",
      "=======   Epoch: 1911 | Average Loss: 0.01281974542874764 | Val Loss: 0.020877217474792684 Sat Jan 27 09:21:48 2024\n",
      "=======   Epoch: 1912 | Average Loss: 0.012801909853009299 | Val Loss: 0.020682584328044738 Sat Jan 27 09:25:15 2024\n",
      "=======   Epoch: 1913 | Average Loss: 0.012787091990319443 | Val Loss: 0.020976609317585826 Sat Jan 27 09:28:47 2024\n",
      "=======   Epoch: 1914 | Average Loss: 0.012799385985714071 | Val Loss: 0.02095685278930302 Sat Jan 27 09:32:19 2024\n",
      "=======   Epoch: 1915 | Average Loss: 0.012791602502315336 | Val Loss: 0.020835069656771208 Sat Jan 27 09:35:46 2024\n",
      "=======   Epoch: 1916 | Average Loss: 0.012829212753209102 | Val Loss: 0.020797331684402058 Sat Jan 27 09:39:17 2024\n",
      "=======   Epoch: 1917 | Average Loss: 0.012806376160946858 | Val Loss: 0.02076599931543959 Sat Jan 27 09:42:49 2024\n",
      "=======   Epoch: 1918 | Average Loss: 0.012789540274175632 | Val Loss: 0.020818711341624812 Sat Jan 27 09:46:17 2024\n",
      "=======   Epoch: 1919 | Average Loss: 0.012795372635373716 | Val Loss: 0.0208362671546638 Sat Jan 27 09:49:48 2024\n",
      "=======   Epoch: 1920 | Average Loss: 0.012779607718917358 | Val Loss: 0.020767660096420774 Sat Jan 27 09:53:20 2024\n",
      "=======   Epoch: 1921 | Average Loss: 0.012808057334656711 | Val Loss: 0.020893053234820918 Sat Jan 27 09:56:48 2024\n",
      "=======   Epoch: 1922 | Average Loss: 0.012790438215448056 | Val Loss: 0.020872991448933526 Sat Jan 27 10:00:21 2024\n",
      "=======   Epoch: 1923 | Average Loss: 0.012780441728130748 | Val Loss: 0.020923543272406926 Sat Jan 27 10:03:52 2024\n",
      "====="
     ]
    }
   ],
   "source": [
    "# from dataset import CityScapeDepth\n",
    "# from model import UNET, DepthEstimationLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "# from utils import transform, add_result, save_checkpoint\n",
    "import os\n",
    "import time\n",
    "\n",
    "def train(checkpoint):\n",
    "    if checkpoint == None:\n",
    "        model = UNET(in_c=3, out_c=1)\n",
    "        start_epoch = 0\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        model = checkpoint['model']\n",
    "\n",
    "    model = model.to(device=device)\n",
    "    criterion = DepthEstimationLoss().to(device=device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-6, amsgrad=True)\n",
    "\n",
    "    print(f\" -- Initiating the Training Process -- Version: {version}\")\n",
    "    print(f\"Epoch: {start_epoch}: \")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        average_loss = 0\n",
    "        model.train()\n",
    "        for i, (image, mask) in enumerate(train_gen):\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred_mask = model(image)\n",
    "            loss = criterion(pred_mask, mask)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            average_loss = average_loss + loss.item()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if i%50 == 0:\n",
    "                print(\"=\", end=\"\")\n",
    "\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        for j, (image, mask) in enumerate(val_gen):\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "            pred_mask = model(image)\n",
    "            loss = criterion(pred_mask, mask)\n",
    "\n",
    "            validation_loss = validation_loss + loss.item()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        save_checkpoint(epoch=epoch, model=model, version=version)\n",
    "        add_result(f\"Epoch: {epoch} | Average Loss: {average_loss/(i + 1)} | Val Loss: {validation_loss/(j + 1)}\", version)\n",
    "        print(f\"   Epoch: {epoch} | Average Loss: {average_loss/(i + 1)} | Val Loss: {validation_loss/(j + 1)} {time.ctime()}\")\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    version = 9\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if f\"Depth_v{version}.pth.tar\" in os.listdir():\n",
    "        checkpoint = f\"Depth_v{version}.pth.tar\"\n",
    "    else:\n",
    "        checkpoint = None\n",
    "    batch_size = 9\n",
    "    workers = 8\n",
    "    epochs = 3000\n",
    "    lr = 1e-5\n",
    "    train_file = \"train.json\"\n",
    "    val_file = \"val.json\"\n",
    "    size = (512, 256)\n",
    "\n",
    "    train_dataset = CityScapeDepth(train_file, size, transform)\n",
    "    val_dataset = CityScapeDepth(val_file, size, transform)\n",
    "\n",
    "    train_gen = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=workers,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=train_dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    val_gen = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=workers,\n",
    "        collate_fn=val_dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    train(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f0663-9f78-4690-8995-910ecb2c499d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1962918-d841-48bc-91cf-838fd45d41ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
